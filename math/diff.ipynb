{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 此文件复现于一位大佬，源文件见[https://github.com/ageron/handson-ml2/blob/master/extra_autodiff.ipynb]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7975a35e0c4d11b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算函数的梯度向量"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8de91771c3a9090"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\dfrac{\\partial f}{\\partial x} = \\displaystyle{\\lim_{\\epsilon \\to 0}}\\dfrac{f(x+\\epsilon, y) - f(x, y)}{\\epsilon}$ ."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c304ed52559db9"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数f在[3, 4]点处，各一阶偏导数为[24.000400000048216, 10.000000000047748]\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def gradients(func: Callable, vars_list, interval=0.0001):\n",
    "    partial_diff = []\n",
    "    func_value = func(*vars_list)\n",
    "    for i in range(len(vars_list)):\n",
    "        # 在这里的[:]是为了防止改变原列表，也可以用copy\n",
    "        delta_var = vars_list[:]\n",
    "        # delta_var = vars_list.copy()\n",
    "        delta_var[i] += interval\n",
    "        delta_value = func(*delta_var)\n",
    "        derivative = (delta_value - func_value) / interval\n",
    "        partial_diff.append(derivative)\n",
    "    return partial_diff\n",
    "\n",
    "\n",
    "def df_2(func, vars_list):\n",
    "    return gradients(func, vars_list)\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return x * x * y + y + 2\n",
    "\n",
    "grad = df_2(f, [3, 4])\n",
    "print(f\"函数f在[3, 4]点处，各一阶偏导数为{grad}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:43:42.691929700Z",
     "start_time": "2024-10-19T03:43:42.655876500Z"
    }
   },
   "id": "9245c407fc244df0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在已知雅可比矩阵后，可以求Hessians矩阵"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc22e84c4e53f9e5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f在[3, 4]处的Hessians矩阵是[[7.999999951380232, 6.000099261882497], [6.000099261882497, -1.4210854715202004e-06]]\n"
     ]
    }
   ],
   "source": [
    "def df_dx(x, y):\n",
    "    return gradients(f, [x, y])[0]\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return gradients(f, [x, y])[1]\n",
    "\n",
    "\n",
    "def hessians(vars_list):\n",
    "    return [gradients(df_dx, vars_list), gradients(df_dy, vars_list)]\n",
    "\n",
    "\n",
    "Hessians = hessians([3, 4])\n",
    "print(f\"f在[3, 4]处的Hessians矩阵是{Hessians}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:43:42.693928500Z",
     "start_time": "2024-10-19T03:43:42.678046400Z"
    }
   },
   "id": "f7ea53805422712a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "那么如何得到精确解呢？？\n",
    "我们可以使用符号计算，通过类和对象表示函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "976960b438057107"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "符号表示，得到的是精确解\n",
      "((X) * (X)) * (Y) + Y + 2\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "print(\"符号表示，得到的是精确解\")\n",
    "# 定义常数类，有值属性\n",
    "class Const(object):\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "    \n",
    "# 定义变量类，有名字和值属性\n",
    "class Var(object):\n",
    "    \n",
    "    def __init__(self, name, value=0):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        \n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    \n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a: Union[Var, Const], b: Union[Var, Const]):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    # 执行数值操作\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    # 人脑写出计算符逻辑\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "    \n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "\n",
    "x = Var(\"X\")\n",
    "y = Var(\"Y\")\n",
    "# f(x,y) = x²y + y + 2\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2)))\n",
    "\"\"\"\n",
    "print(f)时，\n",
    "1. f作为Add的一个实例，执行它的__srt__方法,此时a=Mul(Mul(x, x), y), b=Add(y, Const(2))\n",
    "2. 对a,执行Mul的__str__方法，a=Mul(x, x),b=y\n",
    "3. 执行Mul(x, x)的__str__方法，此时为(x) * (x),返回上一层Mul,则Mul(Mul(x, x), y)结果为((x) * (x)) * (y)\n",
    "4. 同理对于Add(y, Const(2))实例，嵌套得出最终结果\n",
    "5. ((x) * (x)) * (y) + y + 2\n",
    "\n",
    "\"\"\"\n",
    "print(f)\n",
    "\n",
    "# 赋值\n",
    "x.value = 3\n",
    "y.value = 4\n",
    "# 同样的嵌套计算\n",
    "res = f.evaluate()\n",
    "# 函数在(3, 4)处的点\n",
    "print(res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:43:42.730184200Z",
     "start_time": "2024-10-19T03:43:42.693928500Z"
    }
   },
   "id": "5e1695dde3de892b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精确计算雅可比矩阵和黑塞矩阵\n",
      "((x) * (x)) * (0) + ((x) * (1) + (1) * (x)) * (y) + 0 + 0\n",
      "((x) * (x)) * (1) + ((x) * (0) + (0) * (x)) * (y) + 1 + 0\n",
      "24.0 10.0\n",
      "[[8.0, 6.0], [6.0, 0.0]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"精确计算雅可比矩阵和黑塞矩阵\")\n",
    "# 接下来用符号解表示雅可比和Hessians，直接对类加属性\n",
    "# 由于是类中的函数，所以第一个要传入self参数\n",
    "Const.gradients = lambda self, var: Const(0) # 常数导数为0\n",
    "# if self则是对传入的实例进行判断，是不是属于Var的对象\n",
    "Var.gradients = lambda self, var: Const(1) if self is var else Const(0)\n",
    "# + 法的导数，分别求导相加\n",
    "Add.gradients = lambda self, var: Add(self.a.gradients(var), self.b.gradients(var))\n",
    "# 乘法导数，前导后不导，后导前不导\n",
    "Mul.gradients = lambda self, var: Add(Mul(self.a, self.b.gradients(var)), Mul(self.a.gradients(var), self.b))\n",
    "\n",
    "x = Var(name=\"x\", value=3.)\n",
    "y = Var(name=\"y\", value=4.)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "dfdx = f.gradients(x)  # 2xy\n",
    "dfdy = f.gradients(y)  # x² + 1\n",
    "print(dfdx)\n",
    "print(dfdy)\n",
    "# 此时dfdx是Add或其他类的对象\n",
    "a, b = dfdx.evaluate(), dfdy.evaluate()\n",
    "print(a, b)\n",
    "\n",
    "\n",
    "# 计算Hessians矩阵\n",
    "d2fdxdx = dfdx.gradients(x) # 2y\n",
    "d2fdxdy = dfdx.gradients(y) # 2x\n",
    "d2fdydx = dfdy.gradients(x) # 2x\n",
    "d2fdydy = dfdy.gradients(y) # 0\n",
    "Hessians = [[d2fdxdx.evaluate(), d2fdxdy.evaluate()],\n",
    " [d2fdydx.evaluate(), d2fdydy.evaluate()]]\n",
    "print(Hessians)\n",
    "\n",
    "# 常数导数为0\n",
    "c = Const(2)\n",
    "print(c.gradients(c.value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:43:42.731190900Z",
     "start_time": "2024-10-19T03:43:42.712398900Z"
    }
   },
   "id": "5480a8e084202521"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9cb05a09861f2900"
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算反向微分\n",
    "基于链式法则\n",
    "其中，对应的值由前向传播算出"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d49683f6d1a5b4b1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name, init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:48:23.219335100Z",
     "start_time": "2024-10-19T03:48:23.198414500Z"
    }
   },
   "id": "e234ce137e2f0d5e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "x = Var(\"x\", init_value=3)\n",
    "y = Var(\"y\", init_value=4)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y +2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:48:24.588992300Z",
     "start_time": "2024-10-19T03:48:24.573974900Z"
    }
   },
   "id": "cadcda7be30ef2b7"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((x) * (x)) * (y) + y + 2\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:48:30.769100300Z",
     "start_time": "2024-10-19T03:48:30.766100600Z"
    }
   },
   "id": "7faa56337e8239f0"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:48:31.870404900Z",
     "start_time": "2024-10-19T03:48:31.861789600Z"
    }
   },
   "id": "430429b9dd26e540"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "print(x.gradient)\n",
    "print(y.gradient)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:48:35.076319Z",
     "start_time": "2024-10-19T03:48:35.066426500Z"
    }
   },
   "id": "7f02dae74d250f42"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1470c0addb59b79c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用TensorFlow进行自动微分"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef43a10224a4290d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:53:46.456835400Z",
     "start_time": "2024-10-19T03:53:42.346195Z"
    }
   },
   "id": "ddfdfcc188aea402"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=24.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n"
     ]
    }
   ],
   "source": [
    "# 定义变量\n",
    "x = tf.Variable(3.)\n",
    "y = tf.Variable(4.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = x*x*y + y + 2\n",
    "# 计算雅可比矩阵\n",
    "jacobians = tape.gradient(f, [x, y])\n",
    "print(jacobians)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:53:46.516416100Z",
     "start_time": "2024-10-19T03:53:46.454256700Z"
    }
   },
   "id": "9d806073d34ad975"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以计算二阶导数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1f41279788cf730"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "[[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>], [<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, None]]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    f = x*x*y + y + 2\n",
    "    df_dx, df_dy = tape.gradient(f, [x, y])\n",
    "\n",
    "d2f_d2x, d2f_dydx = tape.gradient(df_dx, [x, y])\n",
    "d2f_dxdy, d2f_d2y = tape.gradient(df_dy, [x, y])\n",
    "del tape\n",
    "\n",
    "hessians = [[d2f_d2x, d2f_dydx], [d2f_dxdy, d2f_d2y]]\n",
    "print(hessians)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T03:56:22.129645800Z",
     "start_time": "2024-10-19T03:56:22.096794800Z"
    }
   },
   "id": "dc94366cdd67f8db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算张量相对于它不依赖的变量的导数时，`gradient()` 函数返回 `None`，而不是 0.0。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c3f4445c05740b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
